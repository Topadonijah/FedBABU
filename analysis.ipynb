{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.train_utils import get_data, get_model\n",
    "from models.Update import DatasetSplit\n",
    "from models.test import test_img_local, test_img_local_all, test_img_avg_all, test_img_ensemble_all\n",
    "\n",
    "import pdb\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global/Personalized model analysis (Balanced Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mobile' # cnn, mobile\n",
    "dataset = 'cifar100' # cifar10, cifar100 \n",
    "num_classes = 100 # 10, 100\n",
    "momentum = 0.90\n",
    "wd = 0.0\n",
    "personalization_epoch = 5 # fine-tuning epochs for personalization\n",
    "\n",
    "server_data_ratio = 0.05\n",
    "\n",
    "for shard_per_user in [10]: #[50, 10]: #, 50, 10]: # 10, 5, 2 cifar10 // 100, 50, 10 cifar100\n",
    "    for frac in [0.1]: # 1.0, 0.1\n",
    "        for local_ep in [10]: # 1, 4, 10\n",
    "            for local_upt_part, aggr_part in [('full', 'full'), ('body', 'body')]: # [('body', 'body'), ('head', 'head'), ('full', 'body'), ('full', 'head'), ('full', 'full')]:\n",
    "                args = easydict.EasyDict({'epochs': local_ep,\n",
    "                                          'num_users': 100,\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'server_data_ratio': server_data_ratio,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 50,\n",
    "                                          'bs': 128,\n",
    "                                          'lr': 1e-3,\n",
    "                                          'momentum': momentum,\n",
    "                                          'wd': wd,\n",
    "                                          'split': 'user',\n",
    "                                          'grad_norm': False,\n",
    "                                          'local_ep_pretrain': 0,\n",
    "                                          'lr_decay': 1.0,\n",
    "                                          'model': model,\n",
    "                                          'kernul_num': 9,\n",
    "                                          'kernul_sizes': '3,4,5',\n",
    "                                          'norm': 'batch_norm',\n",
    "                                          'num_filters': 32,\n",
    "                                          'max_pool': 'True',\n",
    "                                          'num_layers_keep': 1,\n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': False,\n",
    "                                          'num_classes': num_classes,\n",
    "                                          'num_channels': 3,\n",
    "                                          'gpu': 1,\n",
    "                                          'stopping_rounds': 10,\n",
    "                                          'verbose': False,\n",
    "                                          'print_freq': 100,\n",
    "                                          'seed': 1,\n",
    "                                          'test_freq': 1,\n",
    "                                          'load_fed': '',\n",
    "                                          'results_save': 'run1', #run1, se1, head_se1, body_se1\n",
    "                                          'start_saving': 0,\n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'aggr_part': aggr_part,\n",
    "                                          'unbalanced': False\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "                base_dir = './save/{}/{}_iid{}_num{}_C{}_le{}_m{}_wd{}/shard{}_sdr{}/{}/'.format(\n",
    "                    args.dataset, args.model, args.iid, args.num_users, args.frac, args.local_ep, args.momentum, args.wd, args.shard_per_user, args.server_data_ratio, args.results_save)\n",
    "#                 base_dir = '/home/osilab7/hdd/jhoon_backup/FL_local_upt_aggr/save/{}/{}_iid{}_num{}_C{}_le{}/shard{}/{}/'.format(\n",
    "#                     args.dataset, args.model, args.iid, args.num_users, args.frac, args.local_ep, args.shard_per_user, args.results_save)\n",
    "                algo_dir = 'local_upt_{}_aggr_{}'.format(args.local_upt_part, args.aggr_part)\n",
    "\n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "                dict_save_path = os.path.join(base_dir, algo_dir, 'dict_users.pkl')\n",
    "                with open(dict_save_path, 'rb') as handle:\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                if args.local_upt_part == 'body':\n",
    "                    body_lr = args.lr\n",
    "                    head_lr = args.lr # (For personalization) \n",
    "                elif args.local_upt_part == 'head':\n",
    "                    body_lr = args.lr # (For personalization)\n",
    "                    head_lr = args.lr\n",
    "                elif args.local_upt_part == 'full':\n",
    "                    body_lr = args.lr\n",
    "                    head_lr = args.lr\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []\n",
    "                after_acc_results = []\n",
    "                \n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_model.pt')\n",
    "#                     model_save_path = os.path.join(base_dir, algo_dir, 'best_local_{}.pt'.format(user))\n",
    "                    net_local.load_state_dict(torch.load(model_save_path), strict=True)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "\n",
    "                    net_local.train()\n",
    "                    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users_train[user]), batch_size=args.local_bs, shuffle=True)\n",
    "\n",
    "                    body_params = [p for name, p in net_local.named_parameters() if 'linear' not in name]\n",
    "                    head_params = [p for name, p in net_local.named_parameters() if 'linear' in name]\n",
    "                    optimizer = torch.optim.SGD([{'params': body_params, 'lr': body_lr},\n",
    "                                                 {'params': head_params, 'lr': head_lr}],\n",
    "                                                momentum=args.momentum)\n",
    "                    \n",
    "                    for iter in range(personalization_epoch):\n",
    "                        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                            images, labels = images.to(args.device), labels.to(args.device)\n",
    "                            net_local.zero_grad()\n",
    "                            logits = net_local(images)\n",
    "\n",
    "                            loss = criterion(logits, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    after_acc_results.append(acc_test)\n",
    "                    \n",
    "                print (\"-----------------------------------------------------\")\n",
    "                print (\"local update part: {}, aggregation part: {}\".format(local_upt_part, aggr_part))\n",
    "                print (\"shard: {}, frac: {}, local_ep: {}\".format(shard_per_user, frac, local_ep))\n",
    "                print (\"Before min/max/mean/std of accuracy\")\n",
    "                print (np.min(before_acc_results), np.max(before_acc_results), np.mean(before_acc_results), round(np.std(before_acc_results), 2))\n",
    "                print (\"After min/max/mean/std of accuracy\")\n",
    "                print (np.min(after_acc_results), np.max(after_acc_results), np.mean(after_acc_results), round(np.std(after_acc_results), 2))\n",
    "                print (\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global/Personalized model analysis (Unbalanced Dataset / IID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mobile' # mobile\n",
    "dataset = 'cifar100' # cifar100 \n",
    "num_classes = 100 # 100\n",
    "\n",
    "for shard_per_user in [2]: # 100, 50, 10 cifar100\n",
    "    for frac in [0.1]: # 1.0, 0.1\n",
    "        for local_ep in [4]: # 4\n",
    "            for local_upt_part, aggr_part in [('body', 'body'), ('full', 'full')]:\n",
    "                args = easydict.EasyDict({'epochs': local_ep,\n",
    "                                          'num_users': 100,\n",
    "                                          'shard_per_user': shard_per_user,\n",
    "                                          'frac': frac,\n",
    "                                          'local_ep': local_ep,\n",
    "                                          'local_bs': 50,\n",
    "                                          'bs': 128,\n",
    "                                          'lr': 1e-3,\n",
    "                                          'momentum': 0.9,\n",
    "                                          'split': 'user',\n",
    "                                          'grad_norm': False,\n",
    "                                          'local_ep_pretrain': 0,\n",
    "                                          'lr_decay': 1.0,\n",
    "                                          'model': model,\n",
    "                                          'kernul_num': 9,\n",
    "                                          'kernul_sizes': '3,4,5',\n",
    "                                          'norm': 'batch_norm',\n",
    "                                          'num_filters': 32,\n",
    "                                          'max_pool': 'True',\n",
    "                                          'num_layers_keep': 1,\n",
    "                                          'dataset': dataset,\n",
    "                                          'iid': True,\n",
    "                                          'num_classes': num_classes,\n",
    "                                          'num_channels': 3,\n",
    "                                          'gpu': 1,\n",
    "                                          'stopping_rounds': 10,\n",
    "                                          'verbose': False,\n",
    "                                          'print_freq': 100,\n",
    "                                          'seed': 1,\n",
    "                                          'test_freq': 1,\n",
    "                                          'load_fed': '',\n",
    "                                          'results_save': 'run1',\n",
    "                                          'start_saving': 0,\n",
    "                                          'local_upt_part': local_upt_part,\n",
    "                                          'aggr_part': aggr_part,\n",
    "                                          'unbalanced': True,\n",
    "                                          'num_batch_users': 25,\n",
    "                                          'moved_data_size': 200,\n",
    "                                          })\n",
    "\n",
    "                # parse args\n",
    "                args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "#                 base_dir = './save/{}/{}_iid{}_num{}_C{}_le{}/shard{}_unbalanced_bu{}_md{}/{}/'.format(\n",
    "#                     args.dataset, args.model, args.iid, args.num_users, args.frac, args.local_ep, args.shard_per_user, args.num_batch_users, args.moved_data_size, args.results_save)\n",
    "                base_dir = '/home/osilab7/hdd/jhoon_backup/FL_local_upt_aggr/save/{}/{}_iid{}_num{}_C{}_le{}/shard{}_unbalanced_bu{}_md{}/{}/'.format(\n",
    "                    args.dataset, args.model, args.iid, args.num_users, args.frac, args.local_ep, args.shard_per_user, args.num_batch_users, args.moved_data_size, args.results_save)\n",
    "                algo_dir = 'local_upt_{}_aggr_{}'.format(args.local_upt_part, args.aggr_part)\n",
    "\n",
    "                dataset_train, dataset_test, dict_users_train, dict_users_test = get_data(args)\n",
    "                dict_save_path = os.path.join(base_dir, algo_dir, 'dict_users.pkl')\n",
    "                with open(dict_save_path, 'rb') as handle:\n",
    "                    dict_users_train, dict_users_test = pickle.load(handle)\n",
    "\n",
    "                # build model\n",
    "                net_glob = get_model(args)\n",
    "                net_glob.train()\n",
    "\n",
    "                net_local_list = []\n",
    "                for user_ix in range(args.num_users):\n",
    "                    net_local_list.append(copy.deepcopy(net_glob))\n",
    "\n",
    "                if args.local_upt_part == 'body':\n",
    "                    body_lr = args.lr\n",
    "                    head_lr = args.lr # (For personalization) \n",
    "                elif args.local_upt_part == 'head':\n",
    "                    body_lr = args.lr # (For personalization)\n",
    "                    head_lr = args.lr\n",
    "                elif args.local_upt_part == 'full':\n",
    "                    body_lr = args.lr\n",
    "                    head_lr = args.lr\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                before_acc_results = []\n",
    "                after_acc_results = []\n",
    "\n",
    "                for user, net_local in enumerate(net_local_list):\n",
    "                    model_save_path = os.path.join(base_dir, algo_dir, 'best_local_{}.pt'.format(user))\n",
    "                    net_local.load_state_dict(torch.load(model_save_path), strict=True)\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    before_acc_results.append(acc_test)\n",
    "\n",
    "                    net_local.train()\n",
    "                    ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users_train[user]), batch_size=args.local_bs, shuffle=True)\n",
    "\n",
    "                    body_params = [p for name, p in net_local.named_parameters() if 'linear' not in name]\n",
    "                    head_params = [p for name, p in net_local.named_parameters() if 'linear' in name]\n",
    "                    optimizer = torch.optim.SGD([{'params': body_params, 'lr': body_lr},\n",
    "                                                 {'params': head_params, 'lr': head_lr}],\n",
    "                                                momentum=0.9)\n",
    "                    \n",
    "                    for iter in range(args.epochs):\n",
    "                        for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                            images, labels = images.to(args.device), labels.to(args.device)\n",
    "                            net_local.zero_grad()\n",
    "                            logits = net_local(images)\n",
    "\n",
    "                            loss = criterion(logits, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    acc_test, loss_test = test_img_local(net_local, dataset_test, args, user_idx=user, idxs=dict_users_test[user])\n",
    "                    after_acc_results.append(acc_test)\n",
    "                    \n",
    "                print (\"-----------------------------------------------------\")\n",
    "                print (\"shard: {}, frac: {}, local_ep: {}\".format(shard_per_user, frac, local_ep))\n",
    "                print (np.mean(before_acc_results), np.mean(after_acc_results))\n",
    "                print (\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
